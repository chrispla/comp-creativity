{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "062ea07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import math\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from diffusers import LMSDiscreteScheduler, StableDiffusionPipeline\n",
    "from IPython import display as IPdisplay\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import logging\n",
    "\n",
    "from chords.infer import predict_chords\n",
    "from flamingo.inference.inference import caption_from_file\n",
    "\n",
    "# load audio\n",
    "sr = 16000\n",
    "y, sr = librosa.load(\"input.mp3\", sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed3e8ef",
   "metadata": {},
   "source": [
    "### Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c2e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into 10 second chunks\n",
    "chunk_length = 10 * sr\n",
    "num_chunks = len(y) // chunk_length\n",
    "y_chunks = []\n",
    "for i in range(num_chunks):\n",
    "    start = i * chunk_length\n",
    "    end = (i + 1) * chunk_length\n",
    "    y_chunks.append(y[start:end])\n",
    "for i, chunk in enumerate(y_chunks):\n",
    "    if not os.path.exists(\"./flamingo/files_to_process\"):\n",
    "        os.makedirs(\"./flamingo/files_to_process\")\n",
    "    sf.write(f\"./flamingo/files_to_process/chunk_{i}.wav\", chunk, sr)\n",
    "\n",
    "# create jsonl file for captioning with audio flamingo\n",
    "caption_prompt = \"Generate a descriptive caption for the following audio. Make sure to comment on its vibe/mood as a music piece\"\n",
    "lines = []\n",
    "for i in range(num_chunks):\n",
    "    lines.append(\n",
    "        {\n",
    "            \"path\": f\"./flamingo/files_to_process/chunk_{i}.wav\",\n",
    "            \"prompt\": caption_prompt,\n",
    "        }\n",
    "    )\n",
    "# write to jsonl file in ./flamingo/files_to_process/inference.jsonl\n",
    "with open(\"./flamingo/files_to_process/inference.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in lines:\n",
    "        f.write(json.dumps(line) + \"\\n\")\n",
    "# caption\n",
    "captions = caption_from_file(\"./flamingo/files_to_process/inference.jsonl\")\n",
    "\n",
    "# Define explicit fallback\n",
    "default_caption = \"Instrumental music passage\"\n",
    "\n",
    "# Process each caption\n",
    "for i in range(len(captions)):\n",
    "    if captions[i] in [\"no response\", \"no caption\", \"\"]:\n",
    "        # First try previous caption\n",
    "        if i > 0 and captions[i - 1] not in [\"no response\", \"no caption\", \"\"]:\n",
    "            captions[i] = captions[i - 1]\n",
    "        # Then try next caption\n",
    "        elif i < len(captions) - 1 and captions[i + 1] not in [\n",
    "            \"no response\",\n",
    "            \"no caption\",\n",
    "            \"\",\n",
    "        ]:\n",
    "            captions[i] = captions[i + 1]\n",
    "        # Use default if no valid neighbors\n",
    "        else:\n",
    "            captions[i] = default_caption\n",
    "\n",
    "# delete folder\n",
    "shutil.rmtree(\"./flamingo/files_to_process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe05d43f",
   "metadata": {},
   "source": [
    "### Major shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd92bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sequences of features over time\n",
    "hop_length = 512 * 8  # Use consistent hop length\n",
    "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=8, hop_length=hop_length)\n",
    "loudness = librosa.feature.rms(y=y, frame_length=2048 * 8, hop_length=hop_length)\n",
    "\n",
    "\n",
    "# get major turn points in these features\n",
    "def detect_significant_changes(mfccs, loudness, window_size=4):\n",
    "    \"\"\"\n",
    "    Detects significant changes in MFCCs and loudness over time using a sliding window.\n",
    "    Returns a dictionary with lists of timestamps for each change type.\n",
    "    \"\"\"\n",
    "    # Dictionary to store timestamps by category - only major changes\n",
    "    changes = {\n",
    "        \"mfcc_increase\": [],\n",
    "        \"mfcc_decrease\": [],\n",
    "        \"rms_increase\": [],\n",
    "        \"rms_decrease\": [],\n",
    "    }\n",
    "\n",
    "    # Define thresholds - only major thresholds\n",
    "    major_mfcc_threshold = 25\n",
    "    major_loudness_threshold = 0.15\n",
    "\n",
    "    # Number of frames to analyze (minimum of both feature arrays)\n",
    "    n_frames = min(mfccs.shape[1], loudness.shape[1])\n",
    "\n",
    "    # Use a sliding window to detect changes\n",
    "    for i in range(window_size, n_frames):\n",
    "        # Calculate change in MFCCs\n",
    "        mfcc_diff = np.mean(np.abs(mfccs[:, i] - mfccs[:, i - window_size]))\n",
    "        mfcc_direction = np.mean(mfccs[:, i]) > np.mean(mfccs[:, i - window_size])\n",
    "\n",
    "        # Track MFCC changes - only major changes\n",
    "        if mfcc_diff > major_mfcc_threshold:\n",
    "            timestamp = frame_to_time(i, sr, hop_length=hop_length)\n",
    "            if mfcc_direction:\n",
    "                changes[\"mfcc_increase\"].append(timestamp)\n",
    "            else:\n",
    "                changes[\"mfcc_decrease\"].append(timestamp)\n",
    "\n",
    "        # Calculate change in loudness\n",
    "        loudness_diff = np.abs(loudness[0, i] - loudness[0, i - window_size])\n",
    "        loudness_direction = loudness[0, i] > loudness[0, i - window_size]\n",
    "\n",
    "        # Track loudness (RMS) changes - only major changes\n",
    "        if loudness_diff > major_loudness_threshold:\n",
    "            timestamp = frame_to_time(i, sr, hop_length=hop_length)\n",
    "            if loudness_direction:\n",
    "                changes[\"rms_increase\"].append(timestamp)\n",
    "            else:\n",
    "                changes[\"rms_decrease\"].append(timestamp)\n",
    "\n",
    "    # Consolidate nearby change points\n",
    "    consolidated_changes = {}\n",
    "    for category in changes.keys():\n",
    "        consolidated_changes[category] = consolidate_timestamps(\n",
    "            changes[category], time_threshold=1.0\n",
    "        )\n",
    "\n",
    "    # Also store the frame numbers for plotting purposes\n",
    "    consolidated_changes[\"frames\"] = {\n",
    "        \"mfcc_increase\": [],\n",
    "        \"mfcc_decrease\": [],\n",
    "        \"rms_increase\": [],\n",
    "        \"rms_decrease\": [],\n",
    "    }\n",
    "\n",
    "    # Convert timestamps back to frame numbers\n",
    "    for category in consolidated_changes.keys():\n",
    "        if category != \"frames\":\n",
    "            consolidated_changes[\"frames\"][category] = [\n",
    "                time_to_frame(t, sr, hop_length=hop_length)\n",
    "                for t in consolidated_changes[category]\n",
    "            ]\n",
    "\n",
    "    return consolidated_changes\n",
    "\n",
    "\n",
    "def consolidate_timestamps(timestamps, time_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Group timestamps that are within time_threshold seconds of each other\n",
    "    and return a single timestamp for each group (the average of the group).\n",
    "    \"\"\"\n",
    "    if not timestamps:\n",
    "        return []\n",
    "\n",
    "    # Sort timestamps\n",
    "    sorted_timestamps = sorted(timestamps)\n",
    "\n",
    "    # Initialize result with the first timestamp\n",
    "    consolidated = []\n",
    "    current_group = [sorted_timestamps[0]]\n",
    "\n",
    "    # Group timestamps that are close to each other\n",
    "    for t in sorted_timestamps[1:]:\n",
    "        if t - current_group[-1] <= time_threshold:\n",
    "            # Add to current group\n",
    "            current_group.append(t)\n",
    "        else:\n",
    "            # Finalize current group and start a new one\n",
    "            consolidated.append(sum(current_group) / len(current_group))\n",
    "            current_group = [t]\n",
    "\n",
    "    # Add the last group\n",
    "    if current_group:\n",
    "        consolidated.append(sum(current_group) / len(current_group))\n",
    "\n",
    "    return consolidated\n",
    "\n",
    "\n",
    "def frame_to_time(frame_number, sr, hop_length=512 * 8):\n",
    "    \"\"\"Convert frame number to time in seconds.\"\"\"\n",
    "    return frame_number * hop_length / sr\n",
    "\n",
    "\n",
    "def time_to_frame(time_sec, sr, hop_length=512 * 8):\n",
    "    \"\"\"Convert time in seconds to frame number.\"\"\"\n",
    "    return int(time_sec * sr / hop_length)\n",
    "\n",
    "\n",
    "changes = detect_significant_changes(mfccs, loudness)\n",
    "\n",
    "# Print the number of detected changes\n",
    "for category, timestamps in changes.items():\n",
    "    if category != \"frames\":\n",
    "        print(f\"{category}: {len(timestamps)} changes\")\n",
    "        if timestamps:\n",
    "            print(f\"  Timestamps: {timestamps}\")\n",
    "\n",
    "# Plot with color-coded change points\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot MFCCs with change points\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title(\"MFCCs with major change points\")\n",
    "plt.imshow(mfccs, aspect=\"auto\", origin=\"lower\")\n",
    "plt.colorbar()\n",
    "\n",
    "# Add MFCC change markers\n",
    "for frame in changes[\"frames\"][\"mfcc_increase\"]:\n",
    "    plt.axvline(x=frame, color=\"red\", linestyle=\"-\", alpha=0.5)\n",
    "for frame in changes[\"frames\"][\"mfcc_decrease\"]:\n",
    "    plt.axvline(x=frame, color=\"blue\", linestyle=\"-\", alpha=0.5)\n",
    "\n",
    "# Plot Loudness with change points\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title(\"Loudness with major change points\")\n",
    "plt.plot(loudness[0])\n",
    "\n",
    "# Add RMS change markers\n",
    "for frame in changes[\"frames\"][\"rms_increase\"]:\n",
    "    plt.axvline(x=frame, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "for frame in changes[\"frames\"][\"rms_decrease\"]:\n",
    "    plt.axvline(x=frame, color=\"blue\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Add a legend\n",
    "custom_lines = [\n",
    "    plt.Line2D([0], [0], color=\"black\", lw=2),\n",
    "    plt.Line2D([0], [0], color=\"red\", linestyle=\"-\", alpha=0.5),\n",
    "    plt.Line2D([0], [0], color=\"blue\", linestyle=\"-\", alpha=0.5),\n",
    "    plt.Line2D([0], [0], color=\"red\", linestyle=\"--\", alpha=0.5),\n",
    "    plt.Line2D([0], [0], color=\"blue\", linestyle=\"--\", alpha=0.5),\n",
    "]\n",
    "\n",
    "plt.figlegend(\n",
    "    custom_lines,\n",
    "    [\n",
    "        \"Loudness\",\n",
    "        \"MFCC Increase\",\n",
    "        \"MFCC Decrease\",\n",
    "        \"Loudness Increase\",\n",
    "        \"Loudness Decrease\",\n",
    "    ],\n",
    "    loc=\"lower center\",\n",
    "    ncol=5,\n",
    "    bbox_to_anchor=(0.5, -0.03),\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927c1e0b",
   "metadata": {},
   "source": [
    "### Chord recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_chords(\"input.mp3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6adbe7",
   "metadata": {},
   "source": [
    "### Image Diffusion and Latent Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319f7981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniconda3/envs/cc/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00,  7.14it/s]\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity_error()\n",
    "\n",
    "model_path = \"nota-ai/bk-sdm-v2-base\"\n",
    "\n",
    "scheduler = LMSDiscreteScheduler(\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    num_train_timesteps=1000,\n",
    ")\n",
    "\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "pipe.scheduler = scheduler\n",
    "\n",
    "\n",
    "def display_images(images, save_path):\n",
    "    \"\"\"Display images in a GIF format.\n",
    "    Args:\n",
    "        images (list): List of images to be displayed.\n",
    "        save_path (str): Path to save the GIF.\n",
    "    Returns:\n",
    "        IPdisplay.Image: Image object for the GIF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert each image in the 'images' list from an array to an Image object.\n",
    "        images = [\n",
    "            Image.fromarray(np.array(image[0], dtype=np.uint8)) for image in images\n",
    "        ]\n",
    "\n",
    "        # Generate a file name based on the current time, replacing colons with hyphens\n",
    "        # to ensure the filename is valid for file systems that don't allow colons.\n",
    "        filename = time.strftime(\"%H:%M:%S\", time.localtime()).replace(\":\", \"-\")\n",
    "        # Save the first image in the list as a GIF file at the 'save_path' location.\n",
    "        # The rest of the images in the list are added as subsequent frames to the GIF.\n",
    "        # The GIF will play each frame for 100 milliseconds and will loop indefinitely.\n",
    "        images[0].save(\n",
    "            f\"{save_path}/{filename}.gif\",\n",
    "            save_all=True,\n",
    "            append_images=images[1:],\n",
    "            duration=100,\n",
    "            loop=0,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return IPdisplay.Image(f\"{save_path}/{filename}.gif\")\n",
    "\n",
    "\n",
    "def slerp(v0, v1, num, t0=0, t1=1):\n",
    "    \"\"\"\n",
    "    Spherical linear interpolation between two vectors v0 and v1.\n",
    "    Args:\n",
    "        v0 (torch.Tensor): Starting vector.\n",
    "        v1 (torch.Tensor): Ending vector.\n",
    "        num (int): Number of interpolation steps.\n",
    "        t0 (float): Start time for interpolation.\n",
    "        t1 (float): End time for interpolation.\n",
    "    Returns:\n",
    "        torch.Tensor: Interpolated vectors.\n",
    "    \"\"\"\n",
    "    # Convert to numpy for calculations\n",
    "    v0 = v0.detach().cpu().numpy()\n",
    "    v1 = v1.detach().cpu().numpy()\n",
    "\n",
    "    def interpolation(t, v0, v1, DOT_THRESHOLD=0.9995):\n",
    "        \"\"\"helper function to spherically interpolate two arrays v1 v2\"\"\"\n",
    "        dot = np.sum(v0 * v1 / (np.linalg.norm(v0) * np.linalg.norm(v1)))\n",
    "        if np.abs(dot) > DOT_THRESHOLD:\n",
    "            v2 = (1 - t) * v0 + t * v1\n",
    "        else:\n",
    "            theta_0 = np.arccos(dot)\n",
    "            sin_theta_0 = np.sin(theta_0)\n",
    "            theta_t = theta_0 * t\n",
    "            sin_theta_t = np.sin(theta_t)\n",
    "            s0 = np.sin(theta_0 - theta_t) / sin_theta_0\n",
    "            s1 = sin_theta_t / sin_theta_0\n",
    "            v2 = s0 * v0 + s1 * v1\n",
    "        return v2\n",
    "\n",
    "    t = np.linspace(t0, t1, num)\n",
    "\n",
    "    # Create interpolated vectors and explicitly convert back to float16 tensor\n",
    "    v3 = torch.tensor(\n",
    "        np.array([interpolation(t[i], v0, v1) for i in range(num)]),\n",
    "        device=\"cuda\",\n",
    "        dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    return v3\n",
    "\n",
    "\n",
    "def generate_interpolated_images(\n",
    "    source_prompt,\n",
    "    target_prompt,\n",
    "    num_interpolation_steps=10,\n",
    "    negative_prompt=None,\n",
    "    height=512,\n",
    "    width=512,\n",
    "    guidance_scale=8,\n",
    "    num_inference_steps=30,\n",
    "    seed=None,\n",
    "    return_gif_path=False,\n",
    "    save_path=\"./\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate images by interpolating between source and target prompts.\n",
    "\n",
    "    Args:\n",
    "        source_prompt (str): Starting prompt\n",
    "        target_prompt (str): Target prompt\n",
    "        num_interpolation_steps (int): Number of interpolation steps\n",
    "        negative_prompt (str or None): Negative prompt for both start and end\n",
    "        height (int): Image height\n",
    "        width (int): Image width\n",
    "        guidance_scale (float): Guidance scale for stable diffusion\n",
    "        num_inference_steps (int): Number of inference steps\n",
    "        seed (int or None): Random seed for generation\n",
    "        return_gif_path (bool): Whether to return the path to the generated GIF\n",
    "        save_path (str): Where to save the images\n",
    "\n",
    "    Returns:\n",
    "        list: List of PIL Image objects if return_gif_path is False,\n",
    "             otherwise tuple of (images, gif_path)\n",
    "    \"\"\"\n",
    "    # Set up generator if seed is provided\n",
    "    if seed is not None:\n",
    "        generator = torch.manual_seed(seed)\n",
    "    else:\n",
    "        generator = None\n",
    "\n",
    "    # Ensure save path exists\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    # Setup prompts list\n",
    "    prompts = [source_prompt, target_prompt]\n",
    "\n",
    "    # Setup negative prompts\n",
    "    if negative_prompt is None:\n",
    "        negative_prompts = [\"\"] * 2\n",
    "    else:\n",
    "        negative_prompts = [negative_prompt] * 2\n",
    "\n",
    "    # Tokenizing and encoding prompts into embeddings\n",
    "    prompts_tokens = pipe.tokenizer(\n",
    "        prompts,\n",
    "        padding=\"max_length\",\n",
    "        max_length=pipe.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    prompts_embeds = pipe.text_encoder(prompts_tokens.input_ids.to(\"cuda\"))[0].half()\n",
    "\n",
    "    # Tokenizing and encoding negative prompts\n",
    "    negative_prompts_tokens = pipe.tokenizer(\n",
    "        negative_prompts,\n",
    "        padding=\"max_length\",\n",
    "        max_length=pipe.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    negative_prompts_embeds = pipe.text_encoder(\n",
    "        negative_prompts_tokens.input_ids.to(\"cuda\")\n",
    "    )[0].half()\n",
    "\n",
    "    # Generate initial latents\n",
    "    latents = torch.randn(\n",
    "        (1, pipe.unet.config.in_channels, height // 8, width // 8),\n",
    "        generator=generator,\n",
    "        device=\"cuda\",\n",
    "        dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    # Generate interpolated embeddings\n",
    "    interpolated_prompt_embeds = slerp(\n",
    "        prompts_embeds[0], prompts_embeds[1], num_interpolation_steps\n",
    "    )\n",
    "\n",
    "    interpolated_negative_prompts_embeds = slerp(\n",
    "        negative_prompts_embeds[0],\n",
    "        negative_prompts_embeds[1],\n",
    "        num_interpolation_steps,\n",
    "    )\n",
    "\n",
    "    # Generate images\n",
    "    images = []\n",
    "    for prompt_embeds, negative_prompt_embeds in tqdm(\n",
    "        zip(interpolated_prompt_embeds, interpolated_negative_prompts_embeds),\n",
    "        total=len(interpolated_prompt_embeds),\n",
    "    ):\n",
    "        images.append(\n",
    "            pipe(\n",
    "                height=height,\n",
    "                width=width,\n",
    "                num_images_per_prompt=1,\n",
    "                prompt_embeds=prompt_embeds[None, ...],\n",
    "                negative_prompt_embeds=negative_prompt_embeds[None, ...],\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                generator=generator,\n",
    "                latents=latents,\n",
    "            ).images\n",
    "        )\n",
    "\n",
    "    # Convert to PIL Images\n",
    "    pil_images = [\n",
    "        Image.fromarray(np.array(image[0], dtype=np.uint8)) for image in images\n",
    "    ]\n",
    "\n",
    "    # Save as GIF if requested\n",
    "    gif_path = None\n",
    "    if save_path:\n",
    "        try:\n",
    "            filename = time.strftime(\"%H:%M:%S\", time.localtime()).replace(\":\", \"-\")\n",
    "            gif_path = f\"{save_path}/{filename}.gif\"\n",
    "            pil_images[0].save(\n",
    "                gif_path,\n",
    "                save_all=True,\n",
    "                append_images=pil_images[1:],\n",
    "                duration=100,\n",
    "                loop=0,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save GIF: {e}\")\n",
    "\n",
    "    if return_gif_path and gif_path:\n",
    "        return pil_images, gif_path\n",
    "    else:\n",
    "        return pil_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e9749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:27<00:00,  2.77s/it]\n"
     ]
    }
   ],
   "source": [
    "images = generate_interpolated_images(\n",
    "    source_prompt=\"A cute dog in a beautiful field of lavender\",\n",
    "    target_prompt=\"A cute cat in a beautiful field of lavender\",\n",
    "    negative_prompt=\"Realistic, detailed\",\n",
    "    num_interpolation_steps=10,\n",
    "    save_path=\"./\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74501cae",
   "metadata": {},
   "source": [
    "### Compose prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b2944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aggregated Events (Quantized at 10 FPS - Frame Indices) ---\n",
      "Frame Index: 0 - Description: Am Piano\n",
      "Frame Index: 2 - Description: Intro music Am Piano\n",
      "Frame Index: 47 - Description: Verse 1 starts Am Piano\n",
      "Frame Index: 50 - Description: Verse 1 starts Gets louder Am Piano\n",
      "Frame Index: 80 - Description: Verse 1 starts Gets louder G Piano\n",
      "Frame Index: 100 - Description: Verse 1 starts Gets louder G Drums enter\n",
      "Frame Index: 120 - Description: Verse 1 starts Gets louder C Drums enter\n",
      "Frame Index: 150 - Description: Verse 1 starts Tempo increase C Drums enter\n",
      "Frame Index: 200 - Description: Verse 1 starts Tempo increase F Guitar solo\n",
      "Frame Index: 250 - Description: Verse 1 starts Gets quieter F Guitar solo\n",
      "Frame Index: 292 - Description: Chorus Gets quieter F Guitar solo\n",
      "Frame Index: 300 - Description: Chorus Gets quieter Am Guitar solo\n",
      "\n",
      "--- Aggregated Events (Original Timestamps) ---\n",
      "Timestamp: 0.00s - Description: Am Piano\n",
      "Timestamp: 0.24s - Description: Intro music Am Piano\n",
      "Timestamp: 4.78s - Description: Verse 1 starts Am Piano\n",
      "Timestamp: 5.00s - Description: Verse 1 starts Gets louder Am Piano\n",
      "Timestamp: 8.00s - Description: Verse 1 starts Gets louder G Piano\n",
      "Timestamp: 10.00s - Description: Verse 1 starts Gets louder G Drums enter\n",
      "Timestamp: 12.00s - Description: Verse 1 starts Gets louder C Drums enter\n",
      "Timestamp: 15.00s - Description: Verse 1 starts Tempo increase C Drums enter\n",
      "Timestamp: 20.00s - Description: Verse 1 starts Tempo increase F Guitar solo\n",
      "Timestamp: 25.00s - Description: Verse 1 starts Gets quieter F Guitar solo\n",
      "Timestamp: 29.24s - Description: Chorus Gets quieter F Guitar solo\n",
      "Timestamp: 30.00s - Description: Chorus Gets quieter Am Guitar solo\n"
     ]
    }
   ],
   "source": [
    "def aggregate_events(*event_lists, fps=None):\n",
    "    \"\"\"\n",
    "    Aggregates events from an arbitrary number of lists based on timestamps,\n",
    "    optionally quantizing timestamps to frame indices based on a given frame rate.\n",
    "\n",
    "    Args:\n",
    "        *event_lists: A variable number of lists, where each list contains\n",
    "                      (timestamp, description) tuples.\n",
    "        fps (float, optional): Frames per second to quantize timestamps to.\n",
    "                               If None, uses original timestamps (in seconds).\n",
    "                               Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list: A sorted list of (key, aggregated_description) tuples.\n",
    "              If fps is provided, key is the frame index (int).\n",
    "              If fps is None, key is the original timestamp (float).\n",
    "    \"\"\"\n",
    "    all_events = []\n",
    "    interval = None\n",
    "    use_frames = False\n",
    "    if fps is not None and fps > 0:\n",
    "        interval = 1.0 / fps\n",
    "        use_frames = True\n",
    "\n",
    "    # Add type identifiers and process timestamps (quantize to frames or keep seconds)\n",
    "    for i, event_list in enumerate(event_lists):\n",
    "        list_id = f\"type{i}\"\n",
    "        for ts, desc in event_list:\n",
    "            # Calculate frame index if fps is provided, otherwise use timestamp\n",
    "            processed_key = ts\n",
    "            if use_frames:\n",
    "                # Calculate frame index\n",
    "                processed_key = math.floor(ts / interval)  # or math.floor(ts * fps)\n",
    "            all_events.append((processed_key, list_id, desc))\n",
    "\n",
    "    # Sort all events by the processed key (frame index or timestamp)\n",
    "    all_events.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Dictionary to keep track of the latest description for each type\n",
    "    num_lists = len(event_lists)\n",
    "\n",
    "    # --- Aggregation using OrderedDict for clean handling of duplicate keys ---\n",
    "    temp_aggregated = collections.OrderedDict()\n",
    "    # Initialize descriptions\n",
    "    current_descriptions = {f\"type{i}\": \"\" for i in range(num_lists)}\n",
    "\n",
    "    # Process events chronologically\n",
    "    last_processed_key = -1  # Keep track of the last key added to output\n",
    "    for key, event_type, description in all_events:\n",
    "        # If the current key is different from the last one processed,\n",
    "        # store the state *before* applying the current event's change.\n",
    "        # This ensures we capture the state at the beginning of each frame/interval.\n",
    "        if key > last_processed_key and last_processed_key >= 0:\n",
    "            aggregated_desc_before = \" \".join(\n",
    "                filter(None, current_descriptions.values())\n",
    "            )\n",
    "            # Avoid adding empty initial state if nothing happened at frame 0 / time 0.0\n",
    "            if aggregated_desc_before or last_processed_key > 0:\n",
    "                temp_aggregated[last_processed_key] = aggregated_desc_before\n",
    "\n",
    "        # Update the description for the current event's type\n",
    "        current_descriptions[event_type] = description\n",
    "        # Store the combined description for the current key (frame or timestamp)\n",
    "        # This overwrites previous entries for the *exact* same key,\n",
    "        # reflecting the latest state at that point.\n",
    "        aggregated_desc_current = \" \".join(filter(None, current_descriptions.values()))\n",
    "        temp_aggregated[key] = aggregated_desc_current\n",
    "        last_processed_key = key\n",
    "\n",
    "    # Convert OrderedDict back to list of tuples\n",
    "    final_aggregated_results = list(temp_aggregated.items())\n",
    "\n",
    "    # Ensure the list is sorted (OrderedDict maintains insertion order,\n",
    "    # but an explicit sort is safer if logic gets complex)\n",
    "    final_aggregated_results.sort(key=lambda x: x[0])\n",
    "\n",
    "    return final_aggregated_results\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# Assume these are your event lists from the different methods\n",
    "events_caption = [\n",
    "    (0.239, \"Intro music\"),\n",
    "    (4.78, \"Verse 1 starts\"),\n",
    "    (29.23876, \"Chorus\"),\n",
    "]\n",
    "events_shifts = [(5.0, \"Gets louder\"), (15.0, \"Tempo increase\"), (25.0, \"Gets quieter\")]\n",
    "events_chords = [(0.0, \"Am\"), (8.0, \"G\"), (12.0, \"C\"), (20.0, \"F\"), (30.0, \"Am\")]\n",
    "events_instruments = [(0.0, \"Piano\"), (10.0, \"Drums enter\"), (20.0, \"Guitar solo\")]\n",
    "\n",
    "\n",
    "# Aggregate the events with 10 FPS quantization (output keys are frame indices)\n",
    "quantization_fps = 8\n",
    "final_events_quantized = aggregate_events(\n",
    "    events_caption,\n",
    "    events_shifts,\n",
    "    events_chords,\n",
    "    events_instruments,\n",
    "    fps=quantization_fps,\n",
    ")\n",
    "\n",
    "# Print the quantized results (frame indices)\n",
    "print(\n",
    "    f\"\\n--- Aggregated Events (Quantized at {quantization_fps} FPS - Frame Indices) ---\"\n",
    ")\n",
    "for frame_idx, desc in final_events_quantized:\n",
    "    print(f\"Frame Index: {frame_idx} - Description: {desc}\")\n",
    "\n",
    "# Example without quantization (output keys are timestamps in seconds)\n",
    "final_events_original = aggregate_events(\n",
    "    events_caption, events_shifts, events_chords, events_instruments\n",
    ")\n",
    "\n",
    "# Print the original results (timestamps)\n",
    "print(\"\\n--- Aggregated Events (Original Timestamps) ---\")\n",
    "for ts, desc in final_events_original:\n",
    "    print(f\"Timestamp: {ts:.2f}s - Description: {desc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
