{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ea07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "from flamingo.inference.inference import caption_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf90aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load audio\n",
    "sr = 16000\n",
    "y, sr = librosa.load(\"input.mp3\", sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed3e8ef",
   "metadata": {},
   "source": [
    "### Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c2e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into 10 second chunks\n",
    "chunk_length = 10 * sr\n",
    "num_chunks = len(y) // chunk_length\n",
    "y_chunks = []\n",
    "for i in range(num_chunks):\n",
    "    start = i * chunk_length\n",
    "    end = (i + 1) * chunk_length\n",
    "    y_chunks.append(y[start:end])\n",
    "for i, chunk in enumerate(y_chunks):\n",
    "    if not os.path.exists(\"./flamingo/files_to_process\"):\n",
    "        os.makedirs(\"./flamingo/files_to_process\")\n",
    "    sf.write(f\"./flamingo/files_to_process/chunk_{i}.wav\", chunk, sr)\n",
    "\n",
    "# create jsonl file for captioning with audio flamingo\n",
    "caption_prompt = \"Generate a descriptive caption for the following audio. Make sure to comment on its vibe/mood as a music piece\"\n",
    "lines = []\n",
    "for i in range(num_chunks):\n",
    "    lines.append(\n",
    "        {\n",
    "            \"path\": f\"./flamingo/files_to_process/chunk_{i}.wav\",\n",
    "            \"prompt\": caption_prompt,\n",
    "        }\n",
    "    )\n",
    "# write to jsonl file in ./flamingo/files_to_process/inference.jsonl\n",
    "with open(\"./flamingo/files_to_process/inference.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in lines:\n",
    "        f.write(json.dumps(line) + \"\\n\")\n",
    "# caption\n",
    "captions = caption_from_file(\"./flamingo/files_to_process/inference.jsonl\")\n",
    "\n",
    "# Define explicit fallback\n",
    "default_caption = \"Instrumental music passage\"\n",
    "\n",
    "# Process each caption\n",
    "for i in range(len(captions)):\n",
    "    if captions[i] in [\"no response\", \"no caption\", \"\"]:\n",
    "        # First try previous caption\n",
    "        if i > 0 and captions[i - 1] not in [\"no response\", \"no caption\", \"\"]:\n",
    "            captions[i] = captions[i - 1]\n",
    "        # Then try next caption\n",
    "        elif i < len(captions) - 1 and captions[i + 1] not in [\n",
    "            \"no response\",\n",
    "            \"no caption\",\n",
    "            \"\",\n",
    "        ]:\n",
    "            captions[i] = captions[i + 1]\n",
    "        # Use default if no valid neighbors\n",
    "        else:\n",
    "            captions[i] = default_caption\n",
    "\n",
    "# delete folder\n",
    "shutil.rmtree(\"./flamingo/files_to_process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe05d43f",
   "metadata": {},
   "source": [
    "### Major shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd92bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sequences of features over time\n",
    "hop_length = 512 * 8  # Use consistent hop length\n",
    "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=8, hop_length=hop_length)\n",
    "loudness = librosa.feature.rms(y=y, frame_length=2048 * 8, hop_length=hop_length)\n",
    "\n",
    "\n",
    "# get major turn points in these features\n",
    "def detect_significant_changes(mfccs, loudness, window_size=4):\n",
    "    \"\"\"\n",
    "    Detects significant changes in MFCCs and loudness over time using a sliding window.\n",
    "    Returns a dictionary with lists of timestamps for each change type.\n",
    "    \"\"\"\n",
    "    # Dictionary to store timestamps by category - only major changes\n",
    "    changes = {\n",
    "        \"mfcc_increase\": [],\n",
    "        \"mfcc_decrease\": [],\n",
    "        \"rms_increase\": [],\n",
    "        \"rms_decrease\": [],\n",
    "    }\n",
    "\n",
    "    # Define thresholds - only major thresholds\n",
    "    major_mfcc_threshold = 25\n",
    "    major_loudness_threshold = 0.15\n",
    "\n",
    "    # Number of frames to analyze (minimum of both feature arrays)\n",
    "    n_frames = min(mfccs.shape[1], loudness.shape[1])\n",
    "\n",
    "    # Use a sliding window to detect changes\n",
    "    for i in range(window_size, n_frames):\n",
    "        # Calculate change in MFCCs\n",
    "        mfcc_diff = np.mean(np.abs(mfccs[:, i] - mfccs[:, i - window_size]))\n",
    "        mfcc_direction = np.mean(mfccs[:, i]) > np.mean(mfccs[:, i - window_size])\n",
    "\n",
    "        # Track MFCC changes - only major changes\n",
    "        if mfcc_diff > major_mfcc_threshold:\n",
    "            timestamp = frame_to_time(i, sr, hop_length=hop_length)\n",
    "            if mfcc_direction:\n",
    "                changes[\"mfcc_increase\"].append(timestamp)\n",
    "            else:\n",
    "                changes[\"mfcc_decrease\"].append(timestamp)\n",
    "\n",
    "        # Calculate change in loudness\n",
    "        loudness_diff = np.abs(loudness[0, i] - loudness[0, i - window_size])\n",
    "        loudness_direction = loudness[0, i] > loudness[0, i - window_size]\n",
    "\n",
    "        # Track loudness (RMS) changes - only major changes\n",
    "        if loudness_diff > major_loudness_threshold:\n",
    "            timestamp = frame_to_time(i, sr, hop_length=hop_length)\n",
    "            if loudness_direction:\n",
    "                changes[\"rms_increase\"].append(timestamp)\n",
    "            else:\n",
    "                changes[\"rms_decrease\"].append(timestamp)\n",
    "\n",
    "    # Consolidate nearby change points\n",
    "    consolidated_changes = {}\n",
    "    for category in changes.keys():\n",
    "        consolidated_changes[category] = consolidate_timestamps(\n",
    "            changes[category], time_threshold=1.0\n",
    "        )\n",
    "\n",
    "    # Also store the frame numbers for plotting purposes\n",
    "    consolidated_changes[\"frames\"] = {\n",
    "        \"mfcc_increase\": [],\n",
    "        \"mfcc_decrease\": [],\n",
    "        \"rms_increase\": [],\n",
    "        \"rms_decrease\": [],\n",
    "    }\n",
    "\n",
    "    # Convert timestamps back to frame numbers\n",
    "    for category in consolidated_changes.keys():\n",
    "        if category != \"frames\":\n",
    "            consolidated_changes[\"frames\"][category] = [\n",
    "                time_to_frame(t, sr, hop_length=hop_length)\n",
    "                for t in consolidated_changes[category]\n",
    "            ]\n",
    "\n",
    "    return consolidated_changes\n",
    "\n",
    "\n",
    "def consolidate_timestamps(timestamps, time_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Group timestamps that are within time_threshold seconds of each other\n",
    "    and return a single timestamp for each group (the average of the group).\n",
    "    \"\"\"\n",
    "    if not timestamps:\n",
    "        return []\n",
    "\n",
    "    # Sort timestamps\n",
    "    sorted_timestamps = sorted(timestamps)\n",
    "\n",
    "    # Initialize result with the first timestamp\n",
    "    consolidated = []\n",
    "    current_group = [sorted_timestamps[0]]\n",
    "\n",
    "    # Group timestamps that are close to each other\n",
    "    for t in sorted_timestamps[1:]:\n",
    "        if t - current_group[-1] <= time_threshold:\n",
    "            # Add to current group\n",
    "            current_group.append(t)\n",
    "        else:\n",
    "            # Finalize current group and start a new one\n",
    "            consolidated.append(sum(current_group) / len(current_group))\n",
    "            current_group = [t]\n",
    "\n",
    "    # Add the last group\n",
    "    if current_group:\n",
    "        consolidated.append(sum(current_group) / len(current_group))\n",
    "\n",
    "    return consolidated\n",
    "\n",
    "\n",
    "def frame_to_time(frame_number, sr, hop_length=512 * 8):\n",
    "    \"\"\"Convert frame number to time in seconds.\"\"\"\n",
    "    return frame_number * hop_length / sr\n",
    "\n",
    "\n",
    "def time_to_frame(time_sec, sr, hop_length=512 * 8):\n",
    "    \"\"\"Convert time in seconds to frame number.\"\"\"\n",
    "    return int(time_sec * sr / hop_length)\n",
    "\n",
    "\n",
    "changes = detect_significant_changes(mfccs, loudness)\n",
    "\n",
    "# Print the number of detected changes\n",
    "for category, timestamps in changes.items():\n",
    "    if category != \"frames\":\n",
    "        print(f\"{category}: {len(timestamps)} changes\")\n",
    "        if timestamps:\n",
    "            print(f\"  Timestamps: {timestamps}\")\n",
    "\n",
    "# Plot with color-coded change points\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot MFCCs with change points\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title(\"MFCCs with major change points\")\n",
    "plt.imshow(mfccs, aspect=\"auto\", origin=\"lower\")\n",
    "plt.colorbar()\n",
    "\n",
    "# Add MFCC change markers\n",
    "for frame in changes[\"frames\"][\"mfcc_increase\"]:\n",
    "    plt.axvline(x=frame, color=\"red\", linestyle=\"-\", alpha=0.5)\n",
    "for frame in changes[\"frames\"][\"mfcc_decrease\"]:\n",
    "    plt.axvline(x=frame, color=\"blue\", linestyle=\"-\", alpha=0.5)\n",
    "\n",
    "# Plot Loudness with change points\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title(\"Loudness with major change points\")\n",
    "plt.plot(loudness[0])\n",
    "\n",
    "# Add RMS change markers\n",
    "for frame in changes[\"frames\"][\"rms_increase\"]:\n",
    "    plt.axvline(x=frame, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "for frame in changes[\"frames\"][\"rms_decrease\"]:\n",
    "    plt.axvline(x=frame, color=\"blue\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Add a legend\n",
    "custom_lines = [\n",
    "    plt.Line2D([0], [0], color=\"black\", lw=2),\n",
    "    plt.Line2D([0], [0], color=\"red\", linestyle=\"-\", alpha=0.5),\n",
    "    plt.Line2D([0], [0], color=\"blue\", linestyle=\"-\", alpha=0.5),\n",
    "    plt.Line2D([0], [0], color=\"red\", linestyle=\"--\", alpha=0.5),\n",
    "    plt.Line2D([0], [0], color=\"blue\", linestyle=\"--\", alpha=0.5),\n",
    "]\n",
    "\n",
    "plt.figlegend(\n",
    "    custom_lines,\n",
    "    [\n",
    "        \"Loudness\",\n",
    "        \"MFCC Increase\",\n",
    "        \"MFCC Decrease\",\n",
    "        \"Loudness Increase\",\n",
    "        \"Loudness Decrease\",\n",
    "    ],\n",
    "    loc=\"lower center\",\n",
    "    ncol=5,\n",
    "    bbox_to_anchor=(0.5, -0.03),\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927c1e0b",
   "metadata": {},
   "source": [
    "### Chord recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chords.infer import predict_chords\n",
    "\n",
    "print(predict_chords(\"input.mp3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6adbe7",
   "metadata": {},
   "source": [
    "### Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319f7981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniconda3/envs/cc/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00,  7.14it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "from IPython import display as IPdisplay\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "model_path = \"nota-ai/bk-sdm-v2-base\"\n",
    "\n",
    "scheduler = LMSDiscreteScheduler(\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    num_train_timesteps=1000,\n",
    ")\n",
    "\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "pipe.scheduler = scheduler\n",
    "\n",
    "\n",
    "def display_images(images, save_path):\n",
    "    \"\"\"Display images in a GIF format.\n",
    "    Args:\n",
    "        images (list): List of images to be displayed.\n",
    "        save_path (str): Path to save the GIF.\n",
    "    Returns:\n",
    "        IPdisplay.Image: Image object for the GIF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert each image in the 'images' list from an array to an Image object.\n",
    "        images = [\n",
    "            Image.fromarray(np.array(image[0], dtype=np.uint8)) for image in images\n",
    "        ]\n",
    "\n",
    "        # Generate a file name based on the current time, replacing colons with hyphens\n",
    "        # to ensure the filename is valid for file systems that don't allow colons.\n",
    "        filename = time.strftime(\"%H:%M:%S\", time.localtime()).replace(\":\", \"-\")\n",
    "        # Save the first image in the list as a GIF file at the 'save_path' location.\n",
    "        # The rest of the images in the list are added as subsequent frames to the GIF.\n",
    "        # The GIF will play each frame for 100 milliseconds and will loop indefinitely.\n",
    "        images[0].save(\n",
    "            f\"{save_path}/{filename}.gif\",\n",
    "            save_all=True,\n",
    "            append_images=images[1:],\n",
    "            duration=100,\n",
    "            loop=0,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return IPdisplay.Image(f\"{save_path}/{filename}.gif\")\n",
    "\n",
    "\n",
    "def slerp(v0, v1, num, t0=0, t1=1):\n",
    "    \"\"\"\n",
    "    Spherical linear interpolation between two vectors v0 and v1.\n",
    "    Args:\n",
    "        v0 (torch.Tensor): Starting vector.\n",
    "        v1 (torch.Tensor): Ending vector.\n",
    "        num (int): Number of interpolation steps.\n",
    "        t0 (float): Start time for interpolation.\n",
    "        t1 (float): End time for interpolation.\n",
    "    Returns:\n",
    "        torch.Tensor: Interpolated vectors.\n",
    "    \"\"\"\n",
    "    # Convert to numpy for calculations\n",
    "    v0 = v0.detach().cpu().numpy()\n",
    "    v1 = v1.detach().cpu().numpy()\n",
    "\n",
    "    def interpolation(t, v0, v1, DOT_THRESHOLD=0.9995):\n",
    "        \"\"\"helper function to spherically interpolate two arrays v1 v2\"\"\"\n",
    "        dot = np.sum(v0 * v1 / (np.linalg.norm(v0) * np.linalg.norm(v1)))\n",
    "        if np.abs(dot) > DOT_THRESHOLD:\n",
    "            v2 = (1 - t) * v0 + t * v1\n",
    "        else:\n",
    "            theta_0 = np.arccos(dot)\n",
    "            sin_theta_0 = np.sin(theta_0)\n",
    "            theta_t = theta_0 * t\n",
    "            sin_theta_t = np.sin(theta_t)\n",
    "            s0 = np.sin(theta_0 - theta_t) / sin_theta_0\n",
    "            s1 = sin_theta_t / sin_theta_0\n",
    "            v2 = s0 * v0 + s1 * v1\n",
    "        return v2\n",
    "\n",
    "    t = np.linspace(t0, t1, num)\n",
    "\n",
    "    # Create interpolated vectors and explicitly convert back to float16 tensor\n",
    "    v3 = torch.tensor(\n",
    "        np.array([interpolation(t[i], v0, v1) for i in range(num)]),\n",
    "        device=\"cuda\",\n",
    "        dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    return v3\n",
    "\n",
    "\n",
    "def generate_interpolated_images(\n",
    "    source_prompt,\n",
    "    target_prompt,\n",
    "    num_interpolation_steps=10,\n",
    "    negative_prompt=None,\n",
    "    height=512,\n",
    "    width=512,\n",
    "    guidance_scale=8,\n",
    "    num_inference_steps=30,\n",
    "    seed=None,\n",
    "    return_gif_path=False,\n",
    "    save_path=\"./\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate images by interpolating between source and target prompts.\n",
    "\n",
    "    Args:\n",
    "        source_prompt (str): Starting prompt\n",
    "        target_prompt (str): Target prompt\n",
    "        num_interpolation_steps (int): Number of interpolation steps\n",
    "        negative_prompt (str or None): Negative prompt for both start and end\n",
    "        height (int): Image height\n",
    "        width (int): Image width\n",
    "        guidance_scale (float): Guidance scale for stable diffusion\n",
    "        num_inference_steps (int): Number of inference steps\n",
    "        seed (int or None): Random seed for generation\n",
    "        return_gif_path (bool): Whether to return the path to the generated GIF\n",
    "        save_path (str): Where to save the images\n",
    "\n",
    "    Returns:\n",
    "        list: List of PIL Image objects if return_gif_path is False,\n",
    "             otherwise tuple of (images, gif_path)\n",
    "    \"\"\"\n",
    "    # Set up generator if seed is provided\n",
    "    if seed is not None:\n",
    "        generator = torch.manual_seed(seed)\n",
    "    else:\n",
    "        generator = None\n",
    "\n",
    "    # Ensure save path exists\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    # Setup prompts list\n",
    "    prompts = [source_prompt, target_prompt]\n",
    "\n",
    "    # Setup negative prompts\n",
    "    if negative_prompt is None:\n",
    "        negative_prompts = [\"\"] * 2\n",
    "    else:\n",
    "        negative_prompts = [negative_prompt] * 2\n",
    "\n",
    "    # Tokenizing and encoding prompts into embeddings\n",
    "    prompts_tokens = pipe.tokenizer(\n",
    "        prompts,\n",
    "        padding=\"max_length\",\n",
    "        max_length=pipe.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    prompts_embeds = pipe.text_encoder(prompts_tokens.input_ids.to(\"cuda\"))[0].half()\n",
    "\n",
    "    # Tokenizing and encoding negative prompts\n",
    "    negative_prompts_tokens = pipe.tokenizer(\n",
    "        negative_prompts,\n",
    "        padding=\"max_length\",\n",
    "        max_length=pipe.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    negative_prompts_embeds = pipe.text_encoder(\n",
    "        negative_prompts_tokens.input_ids.to(\"cuda\")\n",
    "    )[0].half()\n",
    "\n",
    "    # Generate initial latents\n",
    "    latents = torch.randn(\n",
    "        (1, pipe.unet.config.in_channels, height // 8, width // 8),\n",
    "        generator=generator,\n",
    "        device=\"cuda\",\n",
    "        dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    # Generate interpolated embeddings\n",
    "    interpolated_prompt_embeds = slerp(\n",
    "        prompts_embeds[0], prompts_embeds[1], num_interpolation_steps\n",
    "    )\n",
    "\n",
    "    interpolated_negative_prompts_embeds = slerp(\n",
    "        negative_prompts_embeds[0],\n",
    "        negative_prompts_embeds[1],\n",
    "        num_interpolation_steps,\n",
    "    )\n",
    "\n",
    "    # Generate images\n",
    "    images = []\n",
    "    for prompt_embeds, negative_prompt_embeds in tqdm(\n",
    "        zip(interpolated_prompt_embeds, interpolated_negative_prompts_embeds),\n",
    "        total=len(interpolated_prompt_embeds),\n",
    "    ):\n",
    "        images.append(\n",
    "            pipe(\n",
    "                height=height,\n",
    "                width=width,\n",
    "                num_images_per_prompt=1,\n",
    "                prompt_embeds=prompt_embeds[None, ...],\n",
    "                negative_prompt_embeds=negative_prompt_embeds[None, ...],\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                generator=generator,\n",
    "                latents=latents,\n",
    "            ).images\n",
    "        )\n",
    "\n",
    "    # Convert to PIL Images\n",
    "    pil_images = [\n",
    "        Image.fromarray(np.array(image[0], dtype=np.uint8)) for image in images\n",
    "    ]\n",
    "\n",
    "    # Save as GIF if requested\n",
    "    gif_path = None\n",
    "    if save_path:\n",
    "        try:\n",
    "            filename = time.strftime(\"%H:%M:%S\", time.localtime()).replace(\":\", \"-\")\n",
    "            gif_path = f\"{save_path}/{filename}.gif\"\n",
    "            pil_images[0].save(\n",
    "                gif_path,\n",
    "                save_all=True,\n",
    "                append_images=pil_images[1:],\n",
    "                duration=100,\n",
    "                loop=0,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save GIF: {e}\")\n",
    "\n",
    "    if return_gif_path and gif_path:\n",
    "        return pil_images, gif_path\n",
    "    else:\n",
    "        return pil_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e9749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:27<00:00,  2.77s/it]\n"
     ]
    }
   ],
   "source": [
    "images = generate_interpolated_images(\n",
    "    source_prompt=\"A cute dog in a beautiful field of lavender\",\n",
    "    target_prompt=\"A cute cat in a beautiful field of lavender\",\n",
    "    negative_prompt=\"Realistic, detailed\",\n",
    "    num_interpolation_steps=10,\n",
    "    save_path=\"./\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
